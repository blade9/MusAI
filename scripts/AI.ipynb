{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lE1fZCt1bn_U"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "could not convert string to float: '[]'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset_generator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_generator\n\u001b[0;32m      3\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(dataset_generator)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_generator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Jacob\\Documents\\GitHub\\MusAI\\scripts\\dataset_generator.py:51\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# This will be called when the module is reloaded\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Jacob\\Documents\\GitHub\\MusAI\\scripts\\dataset_generator.py:33\u001b[0m, in \u001b[0;36mgenerate_dataset\u001b[1;34m(input_file, batch_size, augment)\u001b[0m\n\u001b[0;32m     30\u001b[0m input_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(input_file)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Load filenames and labels\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m filenames, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Prepend the input directory to the filenames\u001b[39;00m\n\u001b[0;32m     36\u001b[0m filenames \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_dir, filename) \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m filenames]\n",
            "File \u001b[1;32mc:\\Users\\Jacob\\Documents\\GitHub\\MusAI\\scripts\\dataset_generator.py:12\u001b[0m, in \u001b[0;36mload_data_from_file\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m         filename, \u001b[38;5;241m*\u001b[39mlabel_values \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     11\u001b[0m         filenames\u001b[38;5;241m.\u001b[39mappend(filename)\n\u001b[1;32m---> 12\u001b[0m         labels\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m label_values])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filenames, np\u001b[38;5;241m.\u001b[39marray(labels)\n",
            "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '[]'"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "from dataset_generator import dataset_generator\n",
        "importlib.reload(dataset_generator)\n",
        "from scripts.dataset_generator import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JgJNEfQZbtbL"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'create_tf_dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_tf_dataset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrack00001_S00.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'create_tf_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "dataset = create_tf_dataset('Track00001_S00.txt')\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_samples = 10000  # Replace with the actual count\n",
        "train_size = int(0.7 * total_samples)\n",
        "val_size = int(0.15 * total_samples)\n",
        "test_size = total_samples - train_size - val_size\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = dataset.shuffle(buffer_size=10000, reshuffle_each_iteration=False)\n",
        "\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size).take(val_size)\n",
        "test_dataset = dataset.skip(train_size + val_size).take(test_size)\n",
        "\n",
        "train_dataset = train_dataset.repeat().batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.repeat().batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.repeat().batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models.pitch_classifier import build_pitch_classifier_cnn\n",
        "\n",
        "model = build_pitch_classifier_cnn()\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,  # Adjust based on your needs\n",
        "    verbose=1\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
